{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai # to interact with googles code chatbot ai\n",
    "from vertexai.preview.language_models import CodeChatModel, ChatModel, InputOutputTextPair, ChatMessage,CodeGenerationModel, TextGenerationModel\n",
    "import google\n",
    "import openai\n",
    "import openai.error\n",
    "\n",
    "\n",
    "import syfco # to convert between different form\n",
    "import bosy\n",
    "from bosy import BosyError\n",
    "import strix\n",
    "\n",
    "import benchmark\n",
    "from benchmark import Benchmark, build_prompt\n",
    "\n",
    "import verify # used to verify verilog solutions\n",
    "import json\n",
    "import os\n",
    "\n",
    "# both for handling async stuff\n",
    "import asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "import csv # to write the benchmark results to a csv file\n",
    "from utils import *\n",
    "from prompting import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_module_signature(bm: Benchmark, params=None, module_name=\"fsm\"):\n",
    "    if params == None:\n",
    "        params = bm.generate_params\n",
    "    spec = read_file(bm.specification)\n",
    "    inputs = syfco.inputs(spec, overwrite_params=params)\n",
    "    outputs = syfco.outputs(spec, overwrite_params=params)\n",
    "    return f\"\"\"module {module_name} ({\n",
    "    \", \".join(\n",
    "      [\"input \" + inp for inp in inputs] + [\"output reg \" + out for out in outputs]\n",
    "    )\n",
    "  })\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used packages:\n",
    "google-cloud-aiplatform\n",
    "docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"rg-finkbeiner-30141001\", location=\"us-central1\")\n",
    "chat_model = ChatModel.from_pretrained(\"chat-bison\")\n",
    "code_model = CodeChatModel.from_pretrained(\"codechat-bison\")\n",
    "parameters = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_output_tokens\": 1024\n",
    "}\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "def openai_complete(messages, model = \"gpt-3.5-turbo-16k\", retry_n = 10, **kwargs):\n",
    "    count = 0\n",
    "    while count < retry_n:\n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "            )\n",
    "            return completion\n",
    "        except (openai.error.RateLimitError,\n",
    "                openai.error.APIError,\n",
    "                openai.error.APIConnectionError,\n",
    "                openai.error.ServiceUnavailableError,\n",
    "                openai.error.Timeout,\n",
    "                openai.error.TryAgain): # OpenAI API seems to be quite unstable\n",
    "            time.sleep(60)\n",
    "            count = count + 1\n",
    "    raise openai.error.RateLimitError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\n",
    "    Benchmark(bm, \"../../verilog/\") for bm in json.loads(read_file(\"benchmarks.json\"))\n",
    "]\n",
    "benchmarks_strix = [\n",
    "    Benchmark(bm, \"\") for bm in json.loads(read_file(\"benchmarks_strix.json\"))\n",
    "]\n",
    "benchmarks_bosy = [\n",
    "    Benchmark(bm, \"\") for bm in json.loads(read_file(\"benchmarks_bosy.json\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_openai_old(bm : Benchmark, mode = \"self\", template=PromptOpenAI):\n",
    "    prompt_builder = build_prompt(bm=bm, mode=mode, template=template, return_raw=True)\n",
    "    spec = read_file(bm.specification)\n",
    "    \n",
    "    maxval = 0\n",
    "    def complete(messages):\n",
    "      count = 0\n",
    "      while True:\n",
    "        try:\n",
    "          completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\",\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            stop = \"endmodule\",\n",
    "            presence_penalty = -0.5,\n",
    "            n = 3 # best of 3\n",
    "          )\n",
    "          break\n",
    "        except (openai.error.RateLimitError,\n",
    "                openai.error.APIError,\n",
    "                openai.error.APIConnectionError,\n",
    "                openai.error.ServiceUnavailableError,\n",
    "                openai.error.Timeout,\n",
    "                openai.error.TryAgain): # OpenAI API seems to be quite unstable\n",
    "          print(\"trying again \" + str(count))\n",
    "          time.sleep(60)\n",
    "          count = count + 1\n",
    "          if count > 5:\n",
    "            print(bm)\n",
    "            print(\"wtf\")\n",
    "            return \"AI_RATELIMIT\"\n",
    "      \n",
    "      for choice in completion.choices:\n",
    "        response = choice.message.content\n",
    "        code = extract_normalized_verilog_code(response, bm.name)\n",
    "        if code == None:\n",
    "          continue\n",
    "        res = verify.verify_code(bm.specification, code, overwrite_params=bm.generate_params)\n",
    "        if res ==  verify.ReturnCode.SUCCESS:\n",
    "          return code\n",
    "      raise Exception\n",
    "    \n",
    "    params = bm.generate_params\n",
    "    while True:\n",
    "      try:\n",
    "        PARAMS = join_params(params)\n",
    "        SPEC = syfco.convert(spec, \"ltl\", overwrite_params=params)\n",
    "        code = complete(prompt_builder.build_prompt({\n",
    "          \"PARAMS\": PARAMS,\n",
    "          \"SPEC\": SPEC\n",
    "        }))\n",
    "        maxval = next(iter(params.values())) # we only ever use a single parameter, this extracts the current value\n",
    "        key = next(iter(params.keys()))\n",
    "        params = {key: maxval * 2}\n",
    "        prompt_builder.add_example(replacements = {\n",
    "          \"IMPL\": code,\n",
    "          \"PARAMS\": PARAMS,\n",
    "          \"SPEC\": SPEC\n",
    "        })\n",
    "      except:\n",
    "        return maxval\n",
    "\n",
    "def improve_iteratively(response, bm):\n",
    "  code = extract_normalized_verilog_code(response, bm.name)\n",
    "  res = \"NO_CODE\" if code == None else bm.verify(impl=code).name\n",
    "  match res:\n",
    "    case \"NO_CODE\":\n",
    "      prompt = \"This doesn't seem like a complete verilog module. Could you please give me the entire module?\"\n",
    "    case \"ERROR_CONVERT_TO_VERILOG\":\n",
    "      prompt = \"This code isn't syntactically valid verilog code. Could you please fix the syntax errors?\"\n",
    "    case \"ERROR_COMBINE_AIGER\":\n",
    "      prompt = f\"Could you please make sure to use the module signature `{generate_module_signature(bm)}` in your code. Please also try to think about your code again using the new signature. Make sure it matches the specification!\"\n",
    "    case \"FALSE_RESULT\":\n",
    "      prompt = \"The code doesn't satisfy the specification. Please think extensively about how you need to change your code to satisfy the specification. If necessary, rewrite the module from ground up\"\n",
    "    case \"SUCCESS\":\n",
    "      return (code, res)\n",
    "    case _:\n",
    "      return (None, res)\n",
    "  return (prompt, res)\n",
    "\n",
    "\n",
    "def run_single_iterative(bm, type):\n",
    "  # bm.build_prompt might do some heavy work like synthesizing examples with bosy\n",
    "  prompt, examples = build_prompt(bm, params=bm.generate_params, template=PromptPALM, mode=type)\n",
    "  chat = chat_model.start_chat(examples=examples)\n",
    "  response = chat.send_message(prompt, **parameters)\n",
    "  for i in range(0, 3):\n",
    "    (prompt, res) = improve_iteratively(response.text, bm)\n",
    "    if res == \"SUCCESS\" or res == None:\n",
    "      return res\n",
    "    response = chat.send_message(prompt, **parameters)\n",
    "  return res\n",
    "\n",
    "def run_single_explicit_examples(bm, type):\n",
    "    print(\"Starting benchmark \" + bm.name + \"/\" + type)\n",
    "    # bm.build_prompt might do some heavy work like synthesizing examples with bosy\n",
    "    prompt, examples = build_prompt(bm, params=bm.generate_params, template=NoSpecificationPromptPALM, mode=type)\n",
    "    print(\"Finished generating prompt for \" + bm.name + \"/\" + type)\n",
    "    chat = chat_model.start_chat(examples=examples)\n",
    "    response = chat.send_message(prompt, **parameters)\n",
    "    code = extract_normalized_verilog_code(response.text, bm.name)\n",
    "    # print(code if code else \"NO_CODE::\\n\" + response.text)\n",
    "    if code == None:\n",
    "      return \"NO_CODE\"\n",
    "    else:\n",
    "      res = bm.verify(impl=code)\n",
    "      return res.name\n",
    "\n",
    "def run_single_best_k(k):\n",
    "  def run_single (bm, type):\n",
    "    # bm.build_prompt might do some heavy work like synthesizing examples with bosy\n",
    "    prompt, examples = build_prompt(bm, params=bm.generate_params, template=PromptPALM, mode=type)\n",
    "    best = \"NO_CODE\"\n",
    "    for i in range(0, k):\n",
    "      chat = chat_model.start_chat(examples=examples)\n",
    "      response = chat.send_message(prompt, **parameters)\n",
    "      code = extract_normalized_verilog_code(response.text, bm.name)\n",
    "      if code == None:\n",
    "        continue\n",
    "\n",
    "      res = bm.verify(impl=code)\n",
    "      if res ==  verify.ReturnCode.SUCCESS:\n",
    "        return res.name\n",
    "      elif res == verify.ReturnCode.FALSE_RESULT:\n",
    "        best = res.name\n",
    "      elif best != \"FALSE_RESULT\":\n",
    "        best = res.name\n",
    "    return best\n",
    "  return run_single\n",
    "\n",
    "def run_single_template(template=DefaultPromptTemplate):\n",
    "  def run_single(bm, type):\n",
    "    prompt = build_prompt(bm, params=bm.generate_params, template=template, mode=type)\n",
    "    chat = chat_model.start_chat()\n",
    "    response = chat.send_message(prompt, **parameters)\n",
    "    code = extract_normalized_verilog_code(response.text, bm.name)\n",
    "    if code == None:\n",
    "      print(response.text)\n",
    "      return \"NO_CODE\"\n",
    "    else:\n",
    "      res = bm.verify(impl=code)\n",
    "      return res.name\n",
    "  return run_single\n",
    "\n",
    "def run_single_codechat(bm, type, template=DefaultPromptTemplate):\n",
    "  print(\"Starting benchmark \" + bm.name + \"/\" + type)\n",
    "  # bm.build_prompt might do some heavy work like synthesizing examples with bosy\n",
    "  prompt = build_prompt(bm, params=bm.generate_params, template=template, mode=type)\n",
    "  chat = code_model.start_chat()\n",
    "  response = chat.send_message(prompt, **parameters)\n",
    "  code = extract_normalized_verilog_code(response.text, bm.name)\n",
    "  if code == None:\n",
    "    return \"NO_CODE\"\n",
    "  else:\n",
    "    res = bm.verify(impl=code)\n",
    "    return res.name\n",
    "    \n",
    "def run_single_openai(bm, type):\n",
    "  print(\"Starting benchmark \" + bm.name + \"/\" + type)\n",
    "  messages = build_prompt(bm, params=bm.generate_params, template=NoSpecificationPromptOpenAI, mode=type)\n",
    "  print(messages)\n",
    "  \n",
    "  try:\n",
    "    completion = openai_complete(messages=messages)\n",
    "  except openai.error.RateLimitError:\n",
    "    return \"AI_RATELIMIT\"\n",
    "  \n",
    "  response = completion.choices[0].message.content\n",
    "  print(response)\n",
    "  code = extract_normalized_verilog_code(response, bm.name)\n",
    "  if code == None:\n",
    "    return \"NO_CODE\"\n",
    "  else:\n",
    "    res = bm.verify(impl=code, overwrite_params=bm.generate_params)\n",
    "    return res.name\n",
    "\n",
    "def run_single_openai_best_k(k):\n",
    "  def run_single (bm, type):\n",
    "    print(\"Starting benchmark \" + bm.name + \"/\" + type)\n",
    "    messages = build_prompt(bm, params=bm.generate_params, template=PromptOpenAI, mode=type)\n",
    "    \n",
    "    try:\n",
    "      completion = openai_complete(messages=messages, n = k)\n",
    "    except openai.error.RateLimitError:\n",
    "      return \"AI_RATELIMIT\"\n",
    "    \n",
    "    best = \"NO_CODE\"\n",
    "    print(completion)\n",
    "    for choice in completion.choices:\n",
    "      response = choice.message.content\n",
    "      code = extract_normalized_verilog_code(response, bm.name)\n",
    "      if code == None:\n",
    "        continue\n",
    "      res = bm.verify(impl=code)\n",
    "      if res ==  verify.ReturnCode.SUCCESS:\n",
    "        return res.name\n",
    "      elif res == verify.ReturnCode.FALSE_RESULT:\n",
    "        best = res.name\n",
    "      elif best != \"FALSE_RESULT\":\n",
    "        best = res.name\n",
    "    return best\n",
    "  return run_single\n",
    "\n",
    "def find_best_openai(bm : Benchmark, type : str):\n",
    "    INCLUDE_SPECS = True\n",
    "    # prompt_builder = build_prompt(bm, mode=type,template=template,return_raw=True)\n",
    "\n",
    "    parname, curval = list(bm.generate_params.items())[0]\n",
    "    \n",
    "    messages = build_prompt(bm, mode=type,template=PromptOpenAI)\n",
    "    print(\"Starting benchmark: \" + bm.name)\n",
    "\n",
    "    bestval = 0\n",
    "    params = {parname : curval}\n",
    "    while bestval < 1000:\n",
    "      try:\n",
    "        print(\"Current val = \" + str(curval))\n",
    "        completion = openai_complete(messages=messages, n=5)\n",
    "\n",
    "        res = -1\n",
    "        for choice in completion.choices:\n",
    "          response = choice.message.content\n",
    "          code = extract_normalized_verilog_code(response, bm.name)\n",
    "          #print(code)\n",
    "          if code == None:\n",
    "            continue\n",
    "          res = bm.verify(impl=code, overwrite_params=params)\n",
    "          if res ==  verify.ReturnCode.SUCCESS:\n",
    "            break\n",
    "        \n",
    "        if res !=  verify.ReturnCode.SUCCESS: # we got wrong code :(\n",
    "          return str(bestval)\n",
    "        bestval = curval\n",
    "        curval = curval * 2\n",
    "\n",
    "        params = {parname : curval}\n",
    "        messages.append(choice.message.to_dict())\n",
    "        messages.append(\n",
    "          build_prompt(bm, mode=type,template=PromptOpenAI if INCLUDE_SPECS else PromptOpenAINoSpecification, params=params)[-1]\n",
    "        )\n",
    "\n",
    "\n",
    "      except openai.error.RateLimitError: # OpenAI API seems to be quite unstable\n",
    "        return \"AI_RATELIMIT\"\n",
    "      except (openai.InvalidRequestError):\n",
    "        messages = messages[0:1] + messages[2:1] # remove oldest message while keeping system instruction\n",
    "    if code:\n",
    "      print(code)\n",
    "    return str(bestval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Starting benchmark: detector\n",
      "Current val = 8\n",
      "Starting benchmark: detector\n",
      "Current val = 8\n",
      "Starting benchmark: detector\n",
      "Current val = 8\n",
      "Starting benchmark: detector\n",
      "Current val = 8\n",
      "Current val = 16\n",
      "Starting benchmark: mux\n",
      "Current val = 16\n",
      "Starting benchmark: mux\n",
      "Current val = 16\n",
      "Starting benchmark: mux\n",
      "Current val = 16\n",
      "Starting benchmark: mux\n",
      "Current val = 16\n",
      "Current val = 32\n",
      "Current val = 64\n",
      "Current val = 128\n",
      "Current val = 256\n",
      "Current val = 256\n",
      "Current val = 32\n",
      "Current val = 64\n",
      "Current val = 128\n",
      "Current val = 256\n",
      "Current val = 256\n",
      "Starting benchmark: full_arbiter\n",
      "Current val = 4\n",
      "Starting benchmark: full_arbiter\n",
      "Current val = 4\n",
      "Starting benchmark: full_arbiter\n",
      "Current val = 4\n",
      "Starting benchmark: full_arbiter\n",
      "Current val = 4\n",
      "Current val = 8\n",
      "Starting benchmark: simple_arbiter\n",
      "Current val = 8\n",
      "Starting benchmark: simple_arbiter\n",
      "Current val = 8\n",
      "Starting benchmark: simple_arbiter\n",
      "Current val = 8\n",
      "Starting benchmark: simple_arbiter\n",
      "Current val = 8\n",
      "Current val = 16\n",
      "Starting benchmark: shift\n",
      "Current val = 16\n",
      "Starting benchmark: shift\n",
      "Current val = 16\n",
      "Starting benchmark: shift\n",
      "Current val = 16\n",
      "Starting benchmark: shift\n",
      "Current val = 16\n",
      "Current val = 32\n",
      "Current val = 64\n",
      "Current val = 128\n",
      "Current val = 256\n",
      "Current val = 32\n",
      "Current val = 512\n",
      "Current val = 1024\n",
      "Current val = 1024\n",
      "Current val = 32\n",
      "Current val = 64\n",
      "Current val = 64\n",
      "Current val = 128\n",
      "Current val = 128\n",
      "Current val = 256\n",
      "Current val = 512\n",
      "Current val = 512\n",
      "Current val = 256\n",
      "Current val = 256\n"
     ]
    }
   ],
   "source": [
    "async def run_benchmarks(benchmarks, file, example_types = [\"self\", \"bosy\", \"strix\", \"none\"], run_single=run_single_openai):\n",
    "  # setting up csv writing\n",
    "  f = open(file, 'w', newline='')\n",
    "  csvwriter = csv.DictWriter(f, fieldnames=[\"benchmark\"] + example_types, dialect='unix', quoting=csv.QUOTE_NONE)\n",
    "  csvwriter.writeheader()\n",
    "\n",
    "  # get event loop to be able to run the requests in parallel\n",
    "  loop = asyncio.get_event_loop()\n",
    "\n",
    "  async def _run_single(bm, type):\n",
    "    try:\n",
    "      return await loop.run_in_executor(None, run_single, bm, type)\n",
    "    except TimeoutError:\n",
    "      return \"TIMEOUT\"\n",
    "    except BosyError:\n",
    "      return \"BOSY_ERROR\"\n",
    "    except (google.api_core.exceptions.InternalServerError,\n",
    "          openai.InvalidRequestError):\n",
    "      return \"AI_ERROR\" \n",
    "    #except Exception as e:\n",
    "    #  print(e, e.__class__)\n",
    "  async def run_single_benchmark(bm, i = 0):\n",
    "    result = {\n",
    "      \"benchmark\": bm.name\n",
    "    }\n",
    "    print(i)\n",
    "    await asyncio.sleep(300 * i)  # needed because of quota\n",
    "    async_res = await asyncio.gather(\n",
    "      *[_run_single(bm, type) for type in example_types]\n",
    "    )\n",
    "    for i, res in enumerate(async_res):\n",
    "      result[example_types[i]] = res\n",
    "    csvwriter.writerow(result)\n",
    "\n",
    "  await asyncio.gather(\n",
    "    *[run_single_benchmark(bm, i) for i, bm in enumerate(benchmarks)]\n",
    "  )\n",
    "  f.close()\n",
    "\n",
    "await run_benchmarks(benchmarks=benchmarks, run_single=find_best_openai, example_types=[\"self\", \"bosy\", \"strix\", \"none\"],file = \"results/human_highest_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph graphname {\n",
      "\t_init [style=\"invis\"];\n",
      "\t_init -> s0[label=\"\"];\n",
      "\ts0[shape=rectangle,label=\"s0\"];\n",
      "\ts1[shape=rectangle,label=\"s1\"];\n",
      "\ts0 -> s0 [label=\"(¬(r_1 ∧ ¬r_0) ∧ r_1 ∧ r_0) / g\"];\n",
      "\ts0 -> s0 [label=\"(¬(r_1 ∧ ¬r_0) ∧ ¬(r_1 ∧ r_0)) / \"];\n",
      "\ts0 -> s1 [label=\"(r_1 ∧ ¬r_0 ∧ ¬(r_1 ∧ r_0)) / \"];\n",
      "\ts1 -> s0 [label=\"(¬r_1 ∧ r_0) / g\"];\n",
      "\ts1 -> s1 [label=\"(¬(¬r_1 ∧ r_0) ∧ r_0) / g\"];\n",
      "\ts1 -> s1 [label=\"(¬(¬r_1 ∧ r_0) ∧ ¬r_0) / \"];\n",
      "}\n",
      "\n",
      "G (F r_0) && G (F r_1) && G (F r_2) && G (F r_3) <-> G (F g)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dot = bosy.synthesize(read_file(\"detector.tlsf\"), target=\"dot\", overwrite_params={\"n\":2})\n",
    "print(dot)\n",
    "ltl = syfco.convert(read_file(\"detector.tlsf\"), format=\"ltl\", overwrite_params={\"n\": 4})\n",
    "print(ltl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in writing correct verilog code, that fulfill certain formal properties specified in LTL.\n",
      "Here is an example for n=2. It satisfies the LTL specification G (F r_0) && G (F r_1) <-> G (F g):\n",
      "module detector(\n",
      "  input [1:0] r,\n",
      "  input clk,\n",
      "  output reg g\n",
      ");\n",
      "  reg [1:0] state;\n",
      "  initial state = '0;\n",
      "  always @(posedge clk) begin\n",
      "    state = state | r;\n",
      "    g = 0;\n",
      "    if(state == '1) begin\n",
      "      g = 1;\n",
      "      state = '0;\n",
      "    end\n",
      "  end\n",
      "endmodule\n",
      "\n",
      "Please write a Verilog module fulfilling the following expectations. Make sure the code is fully synthesizable.:\n",
      "G (F r_0) && G (F r_1) && G (F r_2) && G (F r_3) <-> G (F g)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "contents = read_file(\"detector.tlsf\")\n",
    "ltl = syfco.convert(contents, \"ltl\", overwrite_params={\"n\": 4})\n",
    "bosy_f = syfco.convert(contents, \"bosy\", overwrite_params={\"n\": 4})\n",
    "bosy_impl = bosy.synthesize(bosy_f)\n",
    "\n",
    "template = PromptTemplate()\n",
    "template.add_example({\n",
    "    \"SPEC\": syfco.convert(contents, \"ltl\", overwrite_params={\"n\": 2}),\n",
    "    \"IMPL\": read_file(\"../../verilog/detector/detector_2.vl\"),\n",
    "    \"PARAMS\": \"n=2\"\n",
    "})\n",
    "prompt = template.build_prompt({\"SPEC\": syfco.convert(contents, \"ltl\", overwrite_params={\"n\": 4})})\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
